{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBFzd4nKexSo",
        "outputId": "382f6a87-6863-48f3-c18b-0a68287b15db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y python-doctr tensorflow tf2onnx\n",
        "!pip install python-doctr[torch]==0.7.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IZ8Y2-DVLQ2T",
        "outputId": "7df69716-4948-4b59-b8df-4d3a0e1119eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping python-doctr as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "\u001b[33mWARNING: Skipping tf2onnx as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting python-doctr==0.7.0 (from python-doctr[torch]==0.7.0)\n",
            "  Downloading python_doctr-0.7.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (8.7.0)\n",
            "Collecting numpy<2.0.0,>=1.16.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.16.2)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.15.1)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (4.12.0.88)\n",
            "Collecting pypdfium2<5.0.0,>=4.0.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyclipper<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.3.0.post6)\n",
            "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.1.2)\n",
            "Collecting langdetect<2.0.0,>=1.0.9 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.10.0)\n",
            "Collecting weasyprint>=55.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading weasyprint-66.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: Pillow>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (11.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (0.7.1)\n",
            "Collecting mplcursors>=0.3 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading mplcursors-0.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting unidecode>=1.0.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr==0.7.0->python-doctr[torch]==0.7.0) (0.36.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr[torch]==0.7.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from python-doctr[torch]==0.7.0) (0.23.0+cu126)\n",
            "Collecting onnx<3.0.0,>=1.12.0 (from python-doctr[torch]==0.7.0)\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect<2.0.0,>=1.0.9->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (0.5.3)\n",
            "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python<5.0.0,>=4.5.0 (from python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (3.4.0)\n",
            "Collecting pydyf>=0.11.0 (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.12/dist-packages (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.0.0)\n",
            "Collecting tinyhtml5>=2.0.0b1 (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.4.0)\n",
            "Collecting cssselect2>=0.8.0 (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen>=0.9.1 (from weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.23.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=0.6->weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.23)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from cssselect2>=0.8.0->weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (0.5.1)\n",
            "Requirement already satisfied: brotli>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from fonttools[woff]>=4.0.0->weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (1.1.0)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint>=55.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0)\n",
            "  Downloading zopfli-0.2.3.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0.0,>=1.12.0->python-doctr[torch]==0.7.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.5.0->python-doctr==0.7.0->python-doctr[torch]==0.7.0) (2025.10.5)\n",
            "Downloading python_doctr-0.7.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mplcursors-0.7-py3-none-any.whl (20 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasyprint-66.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\n",
            "Downloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading zopfli-0.2.3.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (851 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m851.1/851.1 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3fb4004473d4c679d11df195415c0ce5666013e3e092f73cdfa1903756f3d1bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: zopfli, unidecode, tinyhtml5, rapidfuzz, Pyphen, pypdfium2, pydyf, numpy, langdetect, opencv-python, cssselect2, weasyprint, onnx, mplcursors, python-doctr\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pyphen-0.17.2 cssselect2-0.8.0 langdetect-1.0.9 mplcursors-0.7 numpy-1.26.4 onnx-1.19.1 opencv-python-4.11.0.86 pydyf-0.11.0 pypdfium2-4.30.0 python-doctr-0.7.0 rapidfuzz-3.14.1 tinyhtml5-2.0.0 unidecode-1.4.0 weasyprint-66.0 zopfli-0.2.3.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "numpy"
                ]
              },
              "id": "e8517c46203e4b4796cb601fbbb833ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive | grep hindi_oocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8osE6ExIDkS",
        "outputId": "84883e8c-aaa9-4f76-ba51-4a979f18b451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hindi_oocr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive\")  # parent directory of hindi_oocr\n"
      ],
      "metadata": {
        "id": "oyVkCMO8IZ5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hindi_oocr import HindiOCR\n",
        "\n",
        "ocr = HindiOCR(weights_dir=\"/content/drive/MyDrive/hindi_oocr/weights\")"
      ],
      "metadata": {
        "id": "gLNZkRQHIe4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ocr.read(\"/content/essay.png\")\n",
        "print(\"üìù OCR Output:\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi0Oge1zyQee",
        "outputId": "8ab57578-e24d-44e2-f69d-66f46361a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù OCR Output:\n",
            " ‡§π‡•ã‡§≤‡•Ä ‡§´ ‡§´‡§∞‡•ç‡§∑ ‡§∂‡§®‡•Å‡§∞‡§æ‡§ú ‡§µ‡§∏‡§Ç‡§§‡§≤‡•ç‡§≤‡•á ‡§Ü‡§´‡§Æ‡§® ‡§´‡§æ‡§≤‡•ç‡§Æ‡•Å‡§® ‡§´‡•Ä ‡§∞‡§¨‡•ç‡§æ‡§®‡•Ö‡§¶ ‡§ì‡§∞   ‡§â‡§≤‡•ç‡§≤‡§æ‡§∏ ‡§∏‡§æ‡§• ‡§™‡§®\"' ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§´‡§∏‡§≤ ‡§™‡§ï‡§® ‡§ú‡§æ‡§§‡§æ ‡§π ‡§´‡§æ‡§è‡•ç‡§ó‡•Å‡§® ‡§™‡•Ç‡§£‡§Æ‡§æ ‡§ï‡•á ‡§¶‡§ø‡§®‡•ç ‡§≤‡•ã‡§ó ‡§ó‡§ó‡§æ‡§§‡•á ‡§π‡•Ö‡§∏‡§§ - ‡§π‡§Å‡§∏‡§æ‡§§‡•á ‡§Ö‡§™‡§®‡•á ‡§ñ‡•á‡§§‡•á‡§Ç ‡§ú‡§§‡•á ‡§ü‡•à‡§Ç ‡§§‡§π‡§æ‡§Å ‡§∏‡•Å‡§®‡§ü‡§∞‡•Ä ‡§¨‡§æ‡§≤‡§ø‡§Ø‡§æ‡§Å ‡§§‡•ã‡§° ‡§≤‡§æ‡§§‡•á ‡§ê‡•ç‡§∞‡•á‡§Ç ‡§ú‡§¨ ‡§Æ‡•á‡§Ç ‡§Ü‡§ó ‡§§‡§µ‡•ç‡§® ‡§â‡§∏   ‡§Ö‡§ß‡§™‡§ï ‡§Ö‡§®‡•ç‡§® ‡§ï‡•ã ‡§â‡§∏‡§Æ‡•á‡§Ç ‡§≠‡•Ç‡§®‡§Æ‡§∞ ‡§è‡§ï ‡§¨‡§æ‡§∞‡§ï‡•ç‡§∞ 8]{7 ‡§Æ‡§ø‡§≤‡§§ ‡§π‡•á‡§Ç ‡§π‡•ã‡§≤‡§ø‡§ï‡§æ- ‡§¶‡§π‡§® ‡§ï ‡§∏‡§Ç‡§ò‡§Ç‡§ß ‡§π‡•Ö‡•ã‡§≤‡•Ä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/hindi_ocr_output.txt\"\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "print(f\"‚úÖ Text file saved at: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB70zDCmI_oj",
        "outputId": "b151919c-048d-4caf-cbf9-e130796c4cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Text file saved at: /content/hindi_ocr_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **for multiple images**"
      ],
      "metadata": {
        "id": "2jh2wfDlPJjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/imageshandwritten.zip -d /content/Hindi_images\n",
        "print(\"‚úÖ Extracted to /content/Hindi_images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDJPbEE2LKYO",
        "outputId": "67c6ae32-613f-44f8-a88c-eca8d0545ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/imageshandwritten.zip, /content/imageshandwritten.zip.zip or /content/imageshandwritten.zip.ZIP.\n",
            "‚úÖ Extracted to /content/Hindi_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from hindi_oocr import HindiOCR\n",
        "\n",
        "# ‚úÖ Correct folder paths\n",
        "input_folder = \"/content/Hindi_images/imageshandwritten\"\n",
        "output_folder = \"/content/Hindi_texts\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "# ‚úÖ Load your OCR model\n",
        "ocr = HindiOCR(weights_dir=\"/content/drive/MyDrive/hindi_oocr/weights\")"
      ],
      "metadata": {
        "id": "N44rXXpSJ-qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ‚úÖ Process each image and save output\n",
        "for img_file in os.listdir(input_folder):\n",
        "    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        img_path = os.path.join(input_folder, img_file)\n",
        "        print(f\"üîπ Processing: {img_path}\")\n",
        "        text = ocr.read(img_path)\n",
        "\n",
        "        # Save OCR text to a .txt file\n",
        "        txt_name = os.path.splitext(img_file)[0] + \".txt\"\n",
        "        txt_path = os.path.join(output_folder, txt_name)\n",
        "\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "        print(f\"‚úÖ Saved: {txt_path}\")\n",
        "\n",
        "print(\"\\nüéâ All OCR files saved in /content/Hindi_texts/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ecAaHiw0BS-",
        "outputId": "f8779495-ffec-470c-c237-910cede2e287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Processing: /content/Hindi_images/imageshandwritten/Screenshot 2025-10-29 123742.png\n",
            "‚úÖ Saved: /content/Hindi_texts/Screenshot 2025-10-29 123742.txt\n",
            "üîπ Processing: /content/Hindi_images/imageshandwritten/Screenshot 2025-10-29 123313.png\n",
            "‚úÖ Saved: /content/Hindi_texts/Screenshot 2025-10-29 123313.txt\n",
            "\n",
            "üéâ All OCR files saved in /content/Hindi_texts/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/hindi_texts\n"
      ],
      "metadata": {
        "id": "Rqv8GMloKhF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/Hindi_images | head -50\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CoZe8r1Lo5n",
        "outputId": "e4cb3cd6-853d-4d2f-e52e-6a7c1d5a5682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Hindi_images:\n",
            "Hindi_images\n",
            "\n",
            "/content/Hindi_images/Hindi_images:\n",
            "hindihandwritten.png\n",
            "hindiwritten.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layout**"
      ],
      "metadata": {
        "id": "BVrKNi6SPRo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install -y fonts-noto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBZI1H4MLkDi",
        "outputId": "38501d60-1723-4962-9ea8-72ede05a0534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-color-emoji fonts-noto-core\n",
            "  fonts-noto-extra fonts-noto-mono fonts-noto-ui-core fonts-noto-ui-extra\n",
            "  fonts-noto-unhinted\n",
            "The following NEW packages will be installed:\n",
            "  fonts-noto fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-color-emoji\n",
            "  fonts-noto-core fonts-noto-extra fonts-noto-mono fonts-noto-ui-core\n",
            "  fonts-noto-ui-extra fonts-noto-unhinted\n",
            "0 upgraded, 10 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 317 MB of archives.\n",
            "After this operation, 790 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-core all 20201225-1build1 [12.2 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-noto all 20201225-1build1 [16.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk all 1:20220127+repack1-1 [61.2 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk-extra all 1:20220127+repack1-1 [145 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.047-0ubuntu0.22.04.1 [10.0 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-noto-extra all 20201225-1build1 [72.4 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-ui-core all 20201225-1build1 [1,420 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-noto-ui-extra all 20201225-1build1 [14.3 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-noto-unhinted all 20201225-1build1 [16.8 kB]\n",
            "Fetched 317 MB in 4s (84.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-noto-core.\n",
            "(Reading database ... 126455 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-noto-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto.\n",
            "Preparing to unpack .../1-fonts-noto_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "Preparing to unpack .../2-fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Selecting previously unselected package fonts-noto-cjk-extra.\n",
            "Preparing to unpack .../3-fonts-noto-cjk-extra_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk-extra (1:20220127+repack1-1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../4-fonts-noto-color-emoji_2.047-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-noto-extra.\n",
            "Preparing to unpack .../5-fonts-noto-extra_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-extra (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../6-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-ui-core.\n",
            "Preparing to unpack .../7-fonts-noto-ui-core_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-ui-core (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-ui-extra.\n",
            "Preparing to unpack .../8-fonts-noto-ui-extra_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-ui-extra (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-noto-unhinted.\n",
            "Preparing to unpack .../9-fonts-noto-unhinted_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-unhinted (20201225-1build1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Setting up fonts-noto-ui-extra (20201225-1build1) ...\n",
            "Setting up fonts-noto-extra (20201225-1build1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-unhinted (20201225-1build1) ...\n",
            "Setting up fonts-noto-ui-core (20201225-1build1) ...\n",
            "Setting up fonts-noto-core (20201225-1build1) ...\n",
            "Setting up fonts-noto-cjk-extra (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto (20201225-1build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import doctr\n",
        "print(\"‚úÖ DocTR version:\", doctr.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7NbKoLQLz1r",
        "outputId": "a050030c-d631-4298-8dcd-432d34ef550e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DocTR version: v0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from hindi_oocr import HindiOCR\n",
        "\n",
        "# ======== INPUT PATHS ========\n",
        "img_path = \"/content/kavita.png\"   # change as needed\n",
        "json_output_path = \"/content/hindi_layout.json\"\n",
        "\n",
        "# ======== LOAD IMAGE ========\n",
        "img = cv2.imread(img_path)\n",
        "h, w, _ = img.shape\n",
        "\n",
        "# ======== INIT OCR ========\n",
        "ocr = HindiOCR(weights_dir=\"/content/drive/MyDrive/hindi_oocr/weights\")\n",
        "\n",
        "# ======== RUN OCR ========\n",
        "results = reader.readtext(img_path, detail=1, paragraph=False)\n",
        "\n",
        "# ======== PARSE RESULTS ========\n",
        "layout_data = []\n",
        "for (bbox, text, conf) in results:\n",
        "    # bbox = [[x1,y1],[x2,y2],[x3,y3],[x4,y4]]\n",
        "    x_coords = [pt[0] for pt in bbox]\n",
        "    y_coords = [pt[1] for pt in bbox]\n",
        "\n",
        "    x0, y0, x1, y1 = min(x_coords), min(y_coords), max(x_coords), max(y_coords)\n",
        "\n",
        "    # normalize coordinates to [0,1]\n",
        "    x0 /= w; x1 /= w; y0 /= h; y1 /= h\n",
        "\n",
        "    entry = {\n",
        "        \"text\": text.strip(),\n",
        "        \"bbox\": [round(x0,4), round(y0,4), round(x1,4), round(y1,4)],\n",
        "        \"confidence\": round(float(conf), 4)\n",
        "    }\n",
        "    layout_data.append(entry)\n",
        "\n",
        "# ======== SAVE TO JSON ========\n",
        "with open(json_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(layout_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Layout JSON saved to:\", json_output_path)\n",
        "print(f\"üßæ Extracted {len(layout_data)} items\\n\")\n",
        "\n",
        "# print a few samples\n",
        "for l in layout_data[:10]:\n",
        "    print(l)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPW8ubBoZZIX",
        "outputId": "a7a83794-f48c-4890-9125-f6568afabd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Layout JSON saved to: /content/hindi_layout.json\n",
            "üßæ Extracted 21 items\n",
            "\n",
            "{'text': '‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä', 'bbox': [np.float64(0.0324), np.float64(0.0521), np.float64(0.581), np.float64(0.1239)], 'confidence': 0.2519}\n",
            "{'text': '‡§π‡§æ‡§´‡§§‡§æ', 'bbox': [np.float64(0.7086), np.float64(0.0575), np.float64(0.8305), np.float64(0.1077)], 'confidence': 0.6484}\n",
            "{'text': '‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á', 'bbox': [np.float64(0.0438), np.float64(0.1005), np.float64(0.6933), np.float64(0.1813)], 'confidence': 0.1487}\n",
            "{'text': '‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞ ,', 'bbox': [np.float64(0.059), np.float64(0.1741), np.float64(0.6381), np.float64(0.2388)], 'confidence': 0.2722}\n",
            "{'text': '‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§®  ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞,', 'bbox': [np.float64(0.0667), np.float64(0.228), np.float64(0.5543), np.float64(0.3034)], 'confidence': 0.2665}\n",
            "{'text': '‡§π‡•à‡•∞‡•ã ‡§Ü‡§≤‡§æ , ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡•Ç‡§Å ‡§Æ‡•à ‡§á‡§∏‡•ç‡•á,', 'bbox': [np.float64(0.139), np.float64(0.2765), np.float64(0.9562), np.float64(0.3645)], 'confidence': 0.1132}\n",
            "{'text': '‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶', 'bbox': [np.float64(0.0629), np.float64(0.3501), np.float64(0.5238), np.float64(0.4147)], 'confidence': 0.4057}\n",
            "{'text': '‡§≤‡§¨‡§æ‡§¶', 'bbox': [np.float64(0.5905), np.float64(0.3591), np.float64(0.7048), np.float64(0.4093)], 'confidence': 0.822}\n",
            "{'text': '‡§Æ‡•Ä ‡§§‡§∞‡§π', 'bbox': [np.float64(0.7257), np.float64(0.3429), np.float64(0.8762), np.float64(0.4022)], 'confidence': 0.6703}\n",
            "{'text': '‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á', 'bbox': [np.float64(0.1238), np.float64(0.3968), np.float64(0.7162), np.float64(0.4865)], 'confidence': 0.1247}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# CONFIG\n",
        "# =========================\n",
        "JSON_INPUT = \"/content/hindi_layout.json\"          # OCR JSON (EasyOCR / HindiOCR format)\n",
        "OUTPUT_TXT = \"/content/layout_reconstructed.txt\"\n",
        "\n",
        "LINE_Y_THRESHOLD = 0.03       # relative height gap between lines\n",
        "INDENT_MULTIPLIER = 55        # indentation scaling\n",
        "PARAGRAPH_GAP_RATIO = 1.8     # paragraph detection factor\n",
        "\n",
        "# =========================\n",
        "# 1Ô∏è‚É£ LOAD OCR JSON\n",
        "# =========================\n",
        "with open(JSON_INPUT, \"r\", encoding=\"utf-8\") as f:\n",
        "    ocr_data = json.load(f)\n",
        "\n",
        "if not ocr_data:\n",
        "    raise ValueError(\"‚ö†Ô∏è No OCR data found.\")\n",
        "\n",
        "for i, obj in enumerate(ocr_data):\n",
        "    if \"bbox\" not in obj or \"text\" not in obj:\n",
        "        raise ValueError(f\"Invalid OCR entry at index {i}: {obj}\")\n",
        "\n",
        "# =========================\n",
        "# 2Ô∏è‚É£ SORT OCR BY POSITION\n",
        "# =========================\n",
        "ocr_data = sorted(ocr_data, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
        "\n",
        "# =========================\n",
        "# 3Ô∏è‚É£ CLUSTER WORDS INTO LINES\n",
        "# =========================\n",
        "lines, current_line = [], [ocr_data[0]]\n",
        "for i in range(1, len(ocr_data)):\n",
        "    prev_y = np.mean([current_line[-1][\"bbox\"][1], current_line[-1][\"bbox\"][3]])\n",
        "    curr_y = np.mean([ocr_data[i][\"bbox\"][1], ocr_data[i][\"bbox\"][3]])\n",
        "    if abs(curr_y - prev_y) < LINE_Y_THRESHOLD:\n",
        "        current_line.append(ocr_data[i])\n",
        "    else:\n",
        "        lines.append(current_line)\n",
        "        current_line = [ocr_data[i]]\n",
        "lines.append(current_line)\n",
        "\n",
        "# =========================\n",
        "# 4Ô∏è‚É£ CLEAN TEXT PER LINE\n",
        "# =========================\n",
        "clean_lines = []\n",
        "for line in lines:\n",
        "    line = sorted(line, key=lambda x: x[\"bbox\"][0])\n",
        "    raw_text = \" \".join([x[\"text\"] for x in line])\n",
        "    clean_text = re.sub(r\"[^‡§Ä-‡•øA-Za-z0-9\\s‡•§,!?\\\"'\\-]\", \"\", raw_text)\n",
        "    clean_text = re.sub(r\"\\s+\", \" \", clean_text).strip()\n",
        "    clean_lines.append({\n",
        "        \"text\": clean_text,\n",
        "        \"bbox\": np.mean([w[\"bbox\"] for w in line], axis=0).tolist()\n",
        "    })\n",
        "\n",
        "# =========================\n",
        "# 5Ô∏è‚É£ REBUILD LAYOUT TEXT\n",
        "# =========================\n",
        "final_text = \"\"\n",
        "last_y_bottom = 0\n",
        "avg_line_gap = np.median([\n",
        "    clean_lines[i+1][\"bbox\"][1] - clean_lines[i][\"bbox\"][3]\n",
        "    for i in range(len(clean_lines)-1)\n",
        "]) if len(clean_lines) > 1 else 0.03\n",
        "\n",
        "for idx, line in enumerate(clean_lines):\n",
        "    x0, y0, x1, y1 = line[\"bbox\"]\n",
        "    indent_spaces = int(x0 * INDENT_MULTIPLIER)\n",
        "    indent_spaces = max(indent_spaces, 0)\n",
        "    indent = \" \" * indent_spaces\n",
        "\n",
        "    # Detect paragraph break\n",
        "    if idx > 0:\n",
        "        gap = y0 - last_y_bottom\n",
        "        if gap > avg_line_gap * PARAGRAPH_GAP_RATIO:\n",
        "            final_text += \"\\n\"  # paragraph spacing\n",
        "\n",
        "    line_text = line[\"text\"]\n",
        "    if idx < len(clean_lines) - 1 and not re.search(r\"[‡•§!?\\\"']$\", line_text):\n",
        "        line_text = line_text + \" \"  # continuation\n",
        "    final_text += indent + line_text + \"\\n\"\n",
        "    last_y_bottom = y1\n",
        "\n",
        "final_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", final_text).strip()\n",
        "\n",
        "# =========================\n",
        "# 6Ô∏è‚É£ ADVANCED CLEANING\n",
        "# =========================\n",
        "def clean_hindi_text(text):\n",
        "    # Remove roman / numeric noise\n",
        "    text = re.sub(r\"[A-Za-z0-9]+\", \"\", text)\n",
        "\n",
        "    # Keep only Hindi + basic punctuation\n",
        "    text = re.sub(r\"[^‡§Ä-‡•ø‡•§,!? \\n\\-]\", \"\", text)\n",
        "\n",
        "    # Remove isolated matras (vowel signs alone)\n",
        "    text = re.sub(r\"(?<![‡§Ö-‡§π])([‡§æ-‡•å])\", \"\", text)\n",
        "\n",
        "    # Fix spacing and punctuation\n",
        "    text = re.sub(r\"\\s+([‡•§,!?])\", r\"\\1\", text)\n",
        "    text = re.sub(r\"([‡•§!?])([^ \\n])\", r\"\\1 \\2\", text)\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "\n",
        "    # Merge very short broken lines\n",
        "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
        "    merged_lines = []\n",
        "    for i, line in enumerate(lines):\n",
        "        if i > 0 and len(line) < 20:\n",
        "            merged_lines[-1] += \" \" + line\n",
        "        else:\n",
        "            merged_lines.append(line)\n",
        "\n",
        "    # Reconstruct with spacing\n",
        "    text = \"\\n\".join(merged_lines)\n",
        "    return text.strip()\n",
        "\n",
        "final_text = clean_hindi_text(final_text)\n",
        "\n",
        "# =========================\n",
        "# 7Ô∏è‚É£ SAVE OUTPUT\n",
        "# =========================\n",
        "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_text)\n",
        "\n",
        "print(\"‚úÖ Layout-preserved text saved to:\", OUTPUT_TXT)\n",
        "print(\"\\nüß† Cleaned Output Preview:\\n\")\n",
        "print(final_text[:800])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFCFr6kkbrLu",
        "outputId": "0e7f9941-7f8c-45a9-c6ba-5f4a6402628d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Layout-preserved text saved to: /content/layout_reconstructed.txt\n",
            "\n",
            "üß† Cleaned Output Preview:\n",
            "\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä ‡§π‡§æ‡§´‡§§‡§æ\n",
            "‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á\n",
            "‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞,\n",
            "‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞,\n",
            "‡§π‡•à‡•∞ ‡§Ü‡§≤‡§æ, ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡§Å ‡§Æ‡•à ‡§á‡§∏‡•ç,\n",
            "‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶ ‡§≤‡§¨‡§æ‡§¶ ‡§Æ‡•Ä ‡§§‡§∞‡§π\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á\n",
            "‡§¶‡•Å‡§ñ‡§Ç ‡§∏‡•á ‡§™‡§≥‡•Ä ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à ‡§Ø‡•á,\n",
            "‡§¨‡§∏‡§â‡§†‡§æ ‡§∞‡•Ç‡§∞ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§á‡§∏‡•á, ‡§Ö‡§´‡•ç‡§®‡•ã ‡§ú‡§æ‡§∏ ‡§≥‡•™ ‡§∂‡§ö‡§§ ‡§™‡§ø‡§≤‡§æ‡§™‡§æ\n",
            "‡§î‡§∏‡§ì‡§Ç ‡§Ö‡§™‡§®‡•á ‡§Æ‡§≤ ‡§®‡§≤ ‡§®‡§∞‡§≤‡§æ‡§Ø‡§æ\n",
            "‡§´‡§ø‡§∞‡§Å ‡§ù‡§ø‡§™‡§æ ‡§∞‡§Ç‡§™‡•ç‡§§‡•á ‡§§‡§∞‡•ã‡§§‡§æ‡•õ,\n",
            "‡§¨‡§æ‡§≤‡•á ‡§¨‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¨‡§≥‡•á‡§∞‡•á ‡§≥‡•Ä ‡§®‡§∞‡§è\n",
            "‡§∏‡•ç‡§ó‡•ã‡§Ç‡§®‡•á ‡§ì‡§∞‡§®‡§æ ‡§π‡•à ‡§á‡§Ç‡§∏‡•á, ‡§≤ ‡§∏ ‡§∞‡§ï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8Ô∏è‚É£ SMART PARAGRAPH REFORMATTING (Optional)\n",
        "# =========================\n",
        "def smart_paragraph_structure(text):\n",
        "    # Add newlines after Hindi full stops and sentence-ending particles\n",
        "    text = re.sub(r\"([‡•§!?])\", r\"\\1\\n\", text)\n",
        "    text = re.sub(r\"(‡§π‡•à|‡§π‡•à‡§Ç|‡§•‡§æ|‡§•‡•á|‡§π‡•ã‡§§‡§æ ‡§π‡•à|‡§π‡•ã‡§§‡•Ä ‡§π‡•à|‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç|‡§Ü‡§§‡§æ ‡§π‡•à|‡§Æ‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à)\", r\"\\1‡•§\\n\", text)\n",
        "\n",
        "    # Fix spacing before and after punctuation\n",
        "    text = re.sub(r\"\\s+([‡•§!?])\", r\"\\1\", text)\n",
        "    text = re.sub(r\"([‡•§!?])([^ \\n])\", r\"\\1 \\2\", text)\n",
        "\n",
        "    # Normalize excessive newlines and spaces\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "    text = text.strip()\n",
        "\n",
        "    # Add indentation to every paragraph\n",
        "    formatted_lines = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        if len(line.strip()) == 0:\n",
        "            formatted_lines.append(\"\")\n",
        "        else:\n",
        "            formatted_lines.append(\"  \" + line.strip())\n",
        "    return \"\\n\".join(formatted_lines)\n",
        "\n",
        "# Apply paragraph structuring\n",
        "final_text = smart_paragraph_structure(final_text)\n",
        "print(\"\\nüß† Smart Paragraph Structure Output:\\n\")\n",
        "print(final_text[:800])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NntNC4MQdadD",
        "outputId": "a0204688-69ba-4aca-ee72-5247978092cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Smart Paragraph Structure Output:\n",
            "\n",
            "  ‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä ‡§π‡§æ‡§´‡§§‡§æ\n",
            "  ‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á\n",
            "  ‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞,\n",
            "  ‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞,\n",
            "  ‡§π‡•à‡•§\n",
            "  ‡•∞ ‡§Ü‡§≤‡§æ, ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡§Å ‡§Æ‡•à ‡§á‡§∏‡•ç,\n",
            "  ‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶ ‡§≤‡§¨‡§æ‡§¶ ‡§Æ‡•Ä ‡§§‡§∞‡§π\n",
            "  ‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á\n",
            "  ‡§¶‡•Å‡§ñ‡§Ç ‡§∏‡•á ‡§™‡§≥‡•Ä ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à‡•§\n",
            "  ‡§Ø‡•á,\n",
            "  ‡§¨‡§∏‡§â‡§†‡§æ ‡§∞‡•Ç‡§∞ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§á‡§∏‡•á, ‡§Ö‡§´‡•ç‡§®‡•ã ‡§ú‡§æ‡§∏ ‡§≥‡•™ ‡§∂‡§ö‡§§ ‡§™‡§ø‡§≤‡§æ‡§™‡§æ\n",
            "  ‡§î‡§∏‡§ì‡§Ç ‡§Ö‡§™‡§®‡•á ‡§Æ‡§≤ ‡§®‡§≤ ‡§®‡§∞‡§≤‡§æ‡§Ø‡§æ\n",
            "  ‡§´‡§ø‡§∞‡§Å ‡§ù‡§ø‡§™‡§æ ‡§∞‡§Ç‡§™‡•ç‡§§‡•á ‡§§‡§∞‡•ã‡§§‡§æ‡•õ,\n",
            "  ‡§¨‡§æ‡§≤‡•á ‡§¨‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¨‡§≥‡•á‡§∞‡•á ‡§≥‡•Ä ‡§®‡§∞‡§è\n",
            "  ‡§∏‡•ç‡§ó‡•ã‡§Ç‡§®‡•á ‡§ì‡§∞‡§®‡§æ ‡§π‡•à‡•§\n",
            "  ‡§á‡§Ç‡§∏‡•á, ‡§≤ ‡§∏ ‡§∞‡§ï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9Ô∏è‚É£ FINAL POLISHING & NORMALIZATION\n",
        "# =========================\n",
        "def final_polish(text):\n",
        "    # Remove duplicate danda marks\n",
        "    text = re.sub(r\"‡•§\\s*‡•§\", \"‡•§\", text)\n",
        "\n",
        "    # Remove standalone combining characters (junk OCR residuals)\n",
        "    text = re.sub(r\"‡§Ç\\s*\", \"‡§Ç\", text)\n",
        "    text = re.sub(r\"‡§Å\\s*\", \"‡§Å\", text)\n",
        "    text = re.sub(r\"([‡§Ö-‡§π])\\s*([‡§æ-‡•å])\", r\"\\1\\2\", text)\n",
        "\n",
        "    # Remove isolated noise words (like '‡§∞‡§æ', '‡§≤‡§∏', '‡§∏‡§Æ', etc.) heuristically\n",
        "    text = re.sub(r\"\\b(‡§∞‡§æ|‡§≤‡§∏|‡§∏‡§Æ|‡§ñ‡§§‡§æ|‡§¨‡•å‡•õ‡§ï‡•Ä|‡§Ç|‡§¶‡§∞‡§∞|‡§∏‡§µ‡§ß)\\b\", \"\", text)\n",
        "\n",
        "    # Clean broken punctuation\n",
        "    text = re.sub(r\"[!]+\", \"!\", text)\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Add a final newline after paragraph ends\n",
        "    text = re.sub(r\"([‡•§!?])\\s*\\n\", r\"\\1\\n\\n\", text)\n",
        "\n",
        "    # Strip trailing spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "final_text = final_polish(final_text)\n",
        "\n",
        "print(\"\\n‚ú® Final Polished Layout Output:\\n\")\n",
        "print(final_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3f-2Ra_d9Ba",
        "outputId": "e2b460ee-abd3-4ecd-83d1-9a1a44463592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ú® Final Polished Layout Output:\n",
            "\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä ‡§π‡§æ‡§´‡§§‡§æ\n",
            " ‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á\n",
            " ‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞,\n",
            " ‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞,\n",
            " ‡§π‡•à‡•§\n",
            "\n",
            " ‡•∞ ‡§Ü‡§≤‡§æ, ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡§Å‡§Æ‡•à ‡§á‡§∏‡•ç,\n",
            " ‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶ ‡§≤‡§¨‡§æ‡§¶ ‡§Æ‡•Ä ‡§§‡§∞‡§π\n",
            " ‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á\n",
            " ‡§¶‡•Å‡§ñ‡§∏‡•á ‡§™‡§≥‡•Ä ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à‡•§\n",
            "\n",
            " ‡§Ø‡•á,\n",
            " ‡§¨‡§∏‡§â‡§†‡§æ ‡§∞‡•Ç‡§∞ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§á‡§∏‡•á, ‡§Ö‡§´‡•ç‡§®‡•ã ‡§ú‡§æ‡§∏ ‡§≥‡•™ ‡§∂‡§ö‡§§ ‡§™‡§ø‡§≤‡§æ‡§™‡§æ\n",
            " ‡§î‡§∏‡§ì‡§Ö‡§™‡§®‡•á ‡§Æ‡§≤ ‡§®‡§≤ ‡§®‡§∞‡§≤‡§æ‡§Ø‡§æ\n",
            " ‡§´‡§ø‡§∞‡§Å‡§ù‡§ø‡§™‡§æ ‡§∞‡§™‡•ç‡§§‡•á ‡§§‡§∞‡•ã‡§§‡§æ‡•õ,\n",
            " ‡§¨‡§æ‡§≤‡•á ‡§¨‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¨‡§≥‡•á‡§∞‡•á ‡§≥‡•Ä ‡§®‡§∞‡§è\n",
            " ‡§∏‡•ç‡§ó‡•ã‡§Ç‡§®‡•á ‡§ì‡§∞‡§®‡§æ ‡§π‡•à‡•§\n",
            "\n",
            " ‡§á‡§∏‡•á, ‡§≤ ‡§∏ ‡§∞‡§ï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8nfM1uf5eHxL",
        "outputId": "50b2f998-35f7-40a3-d21e-6befa465a393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from indic-nlp-library) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.12/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.4)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.10.5)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "cf7ea23455b2453794d7974100ee6c7e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "# =========================\n",
        "# üî§ Step 10: Devanagari Normalization\n",
        "# =========================\n",
        "def normalize_hindi_text(text):\n",
        "    factory = IndicNormalizerFactory()\n",
        "    normalizer = factory.get_normalizer(\"hi\")\n",
        "    lines = text.split(\"\\n\")\n",
        "    normalized_lines = []\n",
        "    for line in lines:\n",
        "        norm = normalizer.normalize(line)\n",
        "        tokens = indic_tokenize.trivial_tokenize(norm)\n",
        "        norm_line = \" \".join(tokens)\n",
        "        normalized_lines.append(norm_line.strip())\n",
        "    return \"\\n\".join(normalized_lines)\n",
        "\n",
        "final_text = normalize_hindi_text(final_text)\n",
        "print(\"\\nü™∑ Normalized Hindi Output:\\n\")\n",
        "print(final_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSsKlrdUeWNr",
        "outputId": "61f54a9f-0988-43c5-8e63-2e99236ce15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü™∑ Normalized Hindi Output:\n",
            "\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä ‡§π‡§æ‡§´‡§§‡§æ\n",
            "‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á\n",
            "‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞ ,\n",
            "‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞ ,\n",
            "‡§π‡•à ‡•§\n",
            "\n",
            "‡•∞ ‡§Ü‡§≤‡§æ , ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡§Å‡§Æ‡•à ‡§á‡§∏‡•ç ,\n",
            "‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶ ‡§≤‡§¨‡§æ‡§¶ ‡§Æ‡•Ä ‡§§‡§∞‡§π\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á\n",
            "‡§¶‡•Å‡§ñ‡§∏‡•á ‡§™‡§≥‡•Ä ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à ‡•§\n",
            "\n",
            "‡§Ø‡•á ,\n",
            "‡§¨‡§∏‡§â‡§†‡§æ ‡§∞‡•Ç‡§∞ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§á‡§∏‡•á , ‡§Ö‡§´‡•ç‡§®‡•ã ‡§ú‡§æ‡§∏ ‡§≥‡•™ ‡§∂‡§ö‡§§ ‡§™‡§ø‡§≤‡§æ‡§™‡§æ\n",
            "‡§î‡§∏‡§ì‡§Ö‡§™‡§®‡•á ‡§Æ‡§≤ ‡§®‡§≤ ‡§®‡§∞‡§≤‡§æ‡§Ø‡§æ\n",
            "‡§´‡§ø‡§∞‡§Å‡§ù‡§ø‡§™‡§æ ‡§∞‡§™‡•ç‡§§‡•á ‡§§‡§∞‡•ã‡§§‡§æ‡§ú‡§º ,\n",
            "‡§¨‡§æ‡§≤‡•á ‡§¨‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¨‡§≥‡•á‡§∞‡•á ‡§≥‡•Ä ‡§®‡§∞‡§è\n",
            "‡§∏‡•ç‡§ó‡•ã‡§Ç‡§®‡•á ‡§ì‡§∞‡§®‡§æ ‡§π‡•à ‡•§\n",
            "\n",
            "‡§á‡§∏‡•á , ‡§≤ ‡§∏ ‡§∞‡§ï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the text file\n",
        "OUTPUT_TXT = \"/content/final_hindi_text.txt\"\n",
        "\n",
        "# Write the normalized Hindi text to a file\n",
        "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_text)\n",
        "\n",
        "print(f\"‚úÖ Text file saved successfully at: {OUTPUT_TXT}\")\n",
        "\n",
        "# Optional: show preview\n",
        "print(\"\\nüß† Preview of saved file:\\n\")\n",
        "print(final_text[:800])  # print first 800 characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ll76Z2Ceap1",
        "outputId": "03aedf89-3adb-4c26-9e82-3cd1971542d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Text file saved successfully at: /content/final_hindi_text.txt\n",
            "\n",
            "üß† Preview of saved file:\n",
            "\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§¶‡§ø‡§® ‡§Æ‡§∞ ‡§∞‡•Ä‡§•‡§ï‡•Ä ‡§π‡§æ‡§´‡§§‡§æ\n",
            "‡§ú‡§ø‡§¶‡§ó‡•Ä ‡§ú‡•ã ‡§ü‡§æ‡§Å‡§ó ‡§¶‡§ø‡§Ø‡§æ ‡§∞‡•à ‡§Æ‡•à‡§Ç‡§®‡•á\n",
            "‡§ò‡§∞ ‡§®‡•á ‡§ú‡•ã‡§®‡•á ‡§Æ‡•á‡§Ç‡§®‡•ç‡§®‡•á ‡§Ü‡§≤‡•á ‡§™‡§∞ ,\n",
            "‡§∏‡•Å‡§∏‡•ç‡§§‡§æ‡§® ‡§≥‡•á ‡§≤‡§ø‡§Å‡§ó‡•à ‡§™‡§≤ ‡§Æ‡§∞ ,\n",
            "‡§π‡•à ‡•§\n",
            "\n",
            "‡•∞ ‡§Ü‡§≤‡§æ , ‡§ú‡§ø‡§∏ ‡§™‡§∞ ‡§Ö‡§Æ‡•ç‡§∏‡•ç‡§∞ ‡§ü‡§æ‡§Ç‡§ó ‡§¶‡•á‡§§‡§æ ‡§∞‡•ç‡§Å‡§Æ‡•à ‡§á‡§∏‡•ç ,\n",
            "‡§¶‡§ø‡§® ‡§ù‡•ç‡§ü ‡§ì‡§ü‡§®‡•á ‡§¨‡•á ‡§¨‡§æ‡§¶ ‡§≤‡§¨‡§æ‡§¶ ‡§Æ‡•Ä ‡§§‡§∞‡§π\n",
            "‡§Ü‡§ú ‡§´‡§ø‡§∞ ‡§®‡§∑‡•ç‡§ü‡§§ ‡§∂‡§≥‡•Ä ‡§≤‡§ó‡§§ ‡§ê‡•à‡§Ø‡•á\n",
            "‡§¶‡•Å‡§ñ‡§∏‡•á ‡§™‡§≥‡•Ä ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à ‡•§\n",
            "\n",
            "‡§Ø‡•á ,\n",
            "‡§¨‡§∏‡§â‡§†‡§æ ‡§∞‡•Ç‡§∞ ‡§Æ‡•à‡§Ç‡§®‡•á ‡§á‡§∏‡•á , ‡§Ö‡§´‡•ç‡§®‡•ã ‡§ú‡§æ‡§∏ ‡§≥‡•™ ‡§∂‡§ö‡§§ ‡§™‡§ø‡§≤‡§æ‡§™‡§æ\n",
            "‡§î‡§∏‡§ì‡§Ö‡§™‡§®‡•á ‡§Æ‡§≤ ‡§®‡§≤ ‡§®‡§∞‡§≤‡§æ‡§Ø‡§æ\n",
            "‡§´‡§ø‡§∞‡§Å‡§ù‡§ø‡§™‡§æ ‡§∞‡§™‡•ç‡§§‡•á ‡§§‡§∞‡•ã‡§§‡§æ‡§ú‡§º ,\n",
            "‡§¨‡§æ‡§≤‡•á ‡§¨‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§¨‡§≥‡•á‡§∞‡•á ‡§≥‡•Ä ‡§®‡§∞‡§è\n",
            "‡§∏‡•ç‡§ó‡•ã‡§Ç‡§®‡•á ‡§ì‡§∞‡§®‡§æ ‡§π‡•à ‡•§\n",
            "\n",
            "‡§á‡§∏‡•á , ‡§≤ ‡§∏ ‡§∞‡§ï\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "# 1Ô∏è‚É£ Define output path\n",
        "OUTPUT_DOCX = \"/content/final_hindi_text.docx\"\n",
        "\n",
        "# 2Ô∏è‚É£ Create Word document\n",
        "doc = Document()\n",
        "doc.add_paragraph(final_text)\n",
        "\n",
        "# 3Ô∏è‚É£ Save file\n",
        "doc.save(OUTPUT_DOCX)\n",
        "\n",
        "print(f\"‚úÖ Word file created successfully at: {OUTPUT_DOCX}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lq_WUvrpXcO",
        "outputId": "e68b484e-09f5-4502-f5b1-f9139e915d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "‚úÖ Word file created successfully at: /content/final_hindi_text.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install fonts-noto -qq\n",
        "!pip install reportlab\n",
        "\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfbase.ttfonts import TTFont\n",
        "from reportlab.pdfbase import pdfmetrics\n",
        "\n",
        "# 1Ô∏è‚É£ Output path\n",
        "OUTPUT_PDF = \"/content/final_hindi_text.pdf\"\n",
        "\n",
        "# 2Ô∏è‚É£ Register Hindi font\n",
        "pdfmetrics.registerFont(\n",
        "    TTFont('NotoSansDevanagari', '/usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf')\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Create a new PDF canvas\n",
        "c = canvas.Canvas(OUTPUT_PDF, pagesize=A4)\n",
        "c.setFont('NotoSansDevanagari', 14)\n",
        "\n",
        "width, height = A4\n",
        "x_margin, y_start = 70, height - 100\n",
        "line_height = 22\n",
        "y = y_start\n",
        "\n",
        "# 4Ô∏è‚É£ Write each line exactly as in final_text\n",
        "for line in final_text.split(\"\\n\"):\n",
        "    if not line.strip():  # paragraph break\n",
        "        y -= line_height * 1.5\n",
        "        continue\n",
        "    c.drawString(x_margin, y, line)\n",
        "    y -= line_height\n",
        "    if y < 100:  # new page\n",
        "        c.showPage()\n",
        "        c.setFont('NotoSansDevanagari', 14)\n",
        "        y = y_start\n",
        "\n",
        "# 5Ô∏è‚É£ Save PDF\n",
        "c.save()\n",
        "print(f\"‚úÖ PDF file created successfully at: {OUTPUT_PDF}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omnoKb0WqoWI",
        "outputId": "6bb9406e-312d-4237-c76c-ac2417d713e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "‚úÖ PDF file created successfully at: /content/final_hindi_text.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_OlU-X8sG_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}